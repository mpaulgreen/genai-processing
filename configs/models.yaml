default_provider: "openai"
providers:
  claude:
    provider: "anthropic"
    endpoint: "https://api.anthropic.com/v1/messages"
    api_key: "${CLAUDE_API_KEY}"
    model_name: "claude-3-5-sonnet-20241022"
    max_tokens: 4000
    temperature: 0.1
    timeout: "60s"
    retry_attempts: 3
    retry_delay: "1s"
    input_adapter: "claude_input_adapter"
    output_parser: "claude_extractor"
    prompt_formatter: "claude"
    parameters:
      # system: Removed to allow fallback to prompts.yaml base
      max_tokens: "4000"
      temperature: "0.1"

  # OpenAI GPT-4 - Alternative provider for audit queries
  openai:
    provider: "openai"
    endpoint: "https://api.openai.com/v1/chat/completions"
    api_key: "${OPENAI_API_KEY}"
    model_name: "gpt-4"
    max_tokens: 4000
    temperature: 0.1
    timeout: "60s"
    retry_attempts: 3
    retry_delay: "1s"
    input_adapter: "openai_input_adapter"
    output_parser: "openai_extractor"
    prompt_formatter: "openai"
    parameters:
      # system: Removed to allow fallback to prompts.yaml base
      max_tokens: "4000"
      temperature: "0.0"
      presence_penalty: "0.0"   # OpenAI-specific knobs (mapped if present)
      frequency_penalty: "0.0"
      response_format: "json_object"

  # Local Llama 3.1 - For offline/air-gapped environments
  local_llama:
    provider: "ollama"          # Ollama-specific provider
    endpoint: "http://localhost:11434/api/generate"
    api_key: ""                 # No API key needed for local models
    model_name: "llama3.1:8b"
    max_tokens: 4000
    temperature: 0.1
    timeout: "60s"
    retry_attempts: 3
    retry_delay: "1s"
    input_adapter: "ollama_input_adapter"
    output_parser: "ollama_extractor"
    prompt_formatter: "generic"
    parameters:
      # system: Removed to allow fallback to prompts.yaml base
      max_tokens: "4000"
      temperature: "0.1"